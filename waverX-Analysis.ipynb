{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01689f4d-e9b2-43c0-b5a1-409864eaab94",
   "metadata": {},
   "source": [
    "# Climate Wavers Disaster Magnitude Analysis Model\n",
    "##       waverX-Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1751fb89-ac3e-4788-b2ab-adcab93aff6d",
   "metadata": {},
   "source": [
    "## Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "690ab9fd-241c-4004-8d6e-348287488597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==1.3.4 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.3.4)\n",
      "Requirement already satisfied: numpy==1.21.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.21.0)\n",
      "Collecting matplotlib==3.1 (from -r requirements.txt (line 3))\n",
      "  Downloading matplotlib-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m149.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn==1.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (1.0)\n",
      "Requirement already satisfied: joblib==1.3.2 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (1.3.2)\n",
      "Requirement already satisfied: modin in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (0.11.3)\n",
      "Requirement already satisfied: flask==1.1.2 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (1.1.2)\n",
      "Requirement already satisfied: flask_cors==4.0.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (4.0.0)\n",
      "Requirement already satisfied: gunicorn==19.9.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from -r requirements.txt (line 9)) (19.9.0)\n",
      "Requirement already satisfied: requests==2.31.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from -r requirements.txt (line 10)) (2.31.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from pandas==1.3.4->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from pandas==1.3.4->-r requirements.txt (line 1)) (2021.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from matplotlib==3.1->-r requirements.txt (line 3)) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from matplotlib==3.1->-r requirements.txt (line 3)) (1.4.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from matplotlib==3.1->-r requirements.txt (line 3)) (3.0.4)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from scikit-learn==1.0->-r requirements.txt (line 4)) (1.6.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from scikit-learn==1.0->-r requirements.txt (line 4)) (2.2.0)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from flask==1.1.2->-r requirements.txt (line 7)) (2.2.3)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from flask==1.1.2->-r requirements.txt (line 7)) (3.0.1)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from flask==1.1.2->-r requirements.txt (line 7)) (2.1.2)\n",
      "Requirement already satisfied: click>=5.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from flask==1.1.2->-r requirements.txt (line 7)) (8.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from requests==2.31.0->-r requirements.txt (line 10)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from requests==2.31.0->-r requirements.txt (line 10)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from requests==2.31.0->-r requirements.txt (line 10)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from requests==2.31.0->-r requirements.txt (line 10)) (2022.9.14)\n",
      "Requirement already satisfied: packaging in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from modin->-r requirements.txt (line 6)) (21.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from click>=5.1->flask==1.1.2->-r requirements.txt (line 7)) (6.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask==1.1.2->-r requirements.txt (line 7)) (2.1.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib==3.1->-r requirements.txt (line 3)) (4.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas==1.3.4->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages (from importlib-metadata->click>=5.1->flask==1.1.2->-r requirements.txt (line 7)) (3.15.0)\n",
      "Installing collected packages: matplotlib\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.5.3\n",
      "    Uninstalling matplotlib-3.5.3:\n",
      "      Successfully uninstalled matplotlib-3.5.3\n",
      "Successfully installed matplotlib-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ed410a-9525-4032-a540-663e80735bdd",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Historical disasters data from 2000 - 2023 was pulled from the NOAA site to use for this model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21f3083-9fdf-4096-90b5-40b4fae8b59d",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "Historical dataset was cleaned off rows without values for Longitude and Latitude columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9c047271-5156-4889-8509-a6f479b01ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned disasters historical data saved to dataset/historical-disasters.csv\n"
     ]
    }
   ],
   "source": [
    "# Clean the dataset by filtering out rows with missing or empty longitude and latitude values\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def clean_disaster_csv(file):\n",
    "    # Read CSV file into a DataFrame\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Drop rows where longitude or latitude column has missing values\n",
    "    cleaned_df = df.dropna(subset=['Longitude', 'Latitude'])\n",
    "\n",
    "    # Write cleaned DataFrame back to a CSV file\n",
    "    cleaned_df.to_csv(file, index=False)\n",
    "\n",
    "    print(f\"Cleaned disasters historical data saved to {file}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Provide input and output file paths\n",
    "    base_dir = 'dataset/'\n",
    "    disaster_dataset = f\"{base_dir}historical-disasters.csv\"\n",
    "    # Clean the CSV file\n",
    "    clean_disaster_csv(disaster_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847cb0cd-1c4c-478f-bc48-ac8c95cb8ce0",
   "metadata": {},
   "source": [
    "### Dataset Building\n",
    "Dataset was built buy fetching climate data at the time of disasters from visual crossing API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e57a2f-e33b-40b8-9ca5-6986f2c1b1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "import asyncio\n",
    "\n",
    "\"\"\"\n",
    "Fetch climate data at the time of past disasters using the NOAA Climate Data API\n",
    "to use as our training dataset\n",
    "\"\"\"\n",
    "\n",
    "# NOAA Climate Data API URL\n",
    "API_URL = 'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/weatherdata/history/'\n",
    "API_KEY = ''  # API key\n",
    "\n",
    "\n",
    "async def fetch_disasters_climate_data() -> bool:\n",
    "    # Disaster types files to get data\n",
    "    try:\n",
    "        # Check if file alrady exist and continue fetching\n",
    "        climate_dataset_df = pd.read_csv(\"dataset/\" + \"climate_data.csv\")\n",
    "    except Exception:\n",
    "        # Else create a new dataframe\n",
    "        climate_dataset_df = pd.DataFrame([[\"Disaster Type\", \"Magnitude\", \"Magnitude Scale\", \"Minimum Temperature\", \"Maximum Temperature\", \"Dew Point\", \"Temperature\", \"Wind Speed Min\", \"Wind Speed Max\",\n",
    "                                      \"Wind Speed Mean\", \"Wind Direction\", \"Relative Humidity Min\", \"Relative Humidity Max\", \"Relative Humidity Mean\", \"Weather Type\", \"Precipitation\", \"Cloud Cover\", \"Sea Level Pressure\", \"Precipitation Cover\"]])\n",
    "    # Specify the index from which you want to start or continue fetching data\n",
    "    start_index = 778\n",
    "\n",
    "    # Read the CSV file and skip the rows before the start_index\n",
    "    df = pd.read_csv(f'dataset/historical-disasters.csv', skiprows=range(1, start_index))\n",
    "    # Iterate through the DataFrame using iterrows()\n",
    "    climate_dataset_df.to_csv(\"dataset/\" + \"climate_data.csv\", index=False)\n",
    "\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # Access columns using column names\n",
    "        start_year = row['Start Year']\n",
    "        start_month = row['Start Month']\n",
    "        start_day = row['Start Day']\n",
    "        end_year = row['End Year']\n",
    "        end_month = row['End Month']\n",
    "        end_day = row['End Day']\n",
    "        if pd.isna(end_day) or pd.isna(start_day):\n",
    "            continue\n",
    "        if pd.isna(end_month) or pd.isna(start_month):\n",
    "            continue\n",
    "        print(f\"Pulling historical climate data for disaster on {start_year}-{start_month}-{start_day}\")\n",
    "        start_date = \"{}-{:02d}-{:02d}T00:00:00\".format(\n",
    "        start_year, int(start_month), int(start_day))\n",
    "        end_date = \"{}-{:02d}-{:02d}T23:59:59\".format(\n",
    "        end_year, int(end_month), int(end_day))\n",
    "\n",
    "        params = {\n",
    "            # first location of disaster\n",
    "            \"location\": f\"{row['Latitude']},{row['Longitude']}\",\n",
    "            \"startDateTime\": start_date,  # start date of disaster\n",
    "            \"endDateTime\": end_date,  # end date of disaster\n",
    "            \"key\": API_KEY,\n",
    "            \"aggregateHours\": 24,\n",
    "            \"extendedStats\": \"true\",\n",
    "            \"includeAstronomy\": \"true\",\n",
    "            \"contentType\": \"csv\",\n",
    "\n",
    "        }\n",
    "        climate_data_req = requests.get(API_URL, params=params)\n",
    "        if climate_data_req.status_code != 200:\n",
    "            print(\"Failed\")\n",
    "            continue\n",
    "        print(\"Data pulled successfully\")\n",
    "        climate_data_req = climate_data_req.text\n",
    "        climate_data_csv = StringIO(climate_data_req)\n",
    "        data_df = pd.read_csv(climate_data_csv)\n",
    "        # Filter specific columns\n",
    "        filtered_df = data_df[[\"Minimum Temperature\", \"Maximum Temperature\", \"Dew Point\", \"Temperature\", \"Wind Speed Min\", \"Wind Speed Max\",\n",
    "                          \"Wind Speed Mean\", \"Wind Direction\", \"Relative Humidity Min\", \"Relative Humidity Max\", \"Relative Humidity Mean\", \"Weather Type\", \"Precipitation\", \"Cloud Cover\", \"Sea Level Pressure\", \"Precipitation Cover\"]]\n",
    "        filtered_df.insert(0, \"Disaster Type\", row[\"Disaster Type\"])\n",
    "        filtered_df.insert(1, \"Magnitude\", row[\"Magnitude\"])\n",
    "        filtered_df.insert(2, \"Magnitude Scale\", row[\"Magnitude Scale\"])\n",
    "\n",
    "        print(index)\n",
    "        filtered_df.to_csv(\"dataset/\" + \"climate_data.csv\", mode='a', header=False, index=False)\n",
    "\n",
    "\n",
    "    return True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(fetch_disasters_climate_data())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2b3fd7-3cfc-4987-b943-8b8dd1d1736b",
   "metadata": {},
   "source": [
    "## Dataset Cleaning\n",
    "Clean rows with null and empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d54ad564-4d3e-41b7-ba6c-05ccd51fbbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned climate dataset in dataset/climate_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_climate_csv(file):\n",
    "    # Read CSV file into a DataFrame\n",
    "    df = pd.read_csv(file)\n",
    "    # Drop null values\n",
    "    df.dropna(inplace=True)\n",
    "    # Write cleaned DataFrame back to a CSV file\n",
    "    print(f\"Cleaned climate dataset in {file}\")\n",
    "    df.to_csv(file, index=False)\n",
    "\n",
    "    # Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Modify file path to your dataset location  \n",
    "    base_dir = 'dataset/'\n",
    "    clean_climate_csv(f\"{base_dir}climate_data.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f392e5-02cc-42ce-8525-b8d4f67a9287",
   "metadata": {},
   "source": [
    "### Data Balancing\n",
    "Balance disaster magnitude to ensure data magnitude are in same range regardless of their types according to their severity. Flood values in km2 will be divided by highest Km2 value of flood occurred and Storms magnitude in kph will be divided by highest Kph value of storm in dataset and Earthquake richter values divided by 9, highest seen richter value. This values are gotten from analysing data to get the relationship between disaster magnitude scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6fe0f853-16ed-4906-9f0c-c27550879120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data balanced\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"dataset/climate_data.csv\")\n",
    "\n",
    "\n",
    "def balance_data(row):\n",
    "    if row['Magnitude Scale'] == \"Km2\":\n",
    "        return row['Magnitude'] / 1600000\n",
    "    elif row['Magnitude Scale'] == \"Kph\":\n",
    "        return row['Magnitude'] / 300\n",
    "    elif row['Magnitude Scale'] == \"Richter\":\n",
    "        return row['Magnitude'] / 9\n",
    "    else:\n",
    "        return row['Magnitude']\n",
    "\n",
    "df['Magnitude'] = df.apply(lambda row: balance_data(row), axis=1)\n",
    "\n",
    "df.reset_index(drop=True)\n",
    "\n",
    "df.to_csv(\"dataset/balanced_climate_data.csv\", index=False)\n",
    "\n",
    "print(\"Data balanced\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018bec25-393d-48d6-848a-337d8d5f3f44",
   "metadata": {},
   "source": [
    "## Model Building and Optimizing with Intel Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "59c20207-c789-4c90-83c9-2cf041891f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00659641 0.06230194 0.03420891 ... 0.00931116 0.01737112 0.06194267]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import modin.pandas as pd\n",
    "import modin.config as cfg\n",
    "import numpy as np\n",
    "cfg.Engine.put('Ray') \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearnex import patch_sklearn\n",
    "from sklearn import config_context\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "patch_sklearn()\n",
    "\n",
    "# Load disaster climate data\n",
    "climate_df = pd.read_csv(\"dataset/balanced_climate_data.csv\")\n",
    "\n",
    "# Drop NaN values\n",
    "climate_df.dropna(inplace=True)\n",
    "\n",
    "# Encode  string values\n",
    "label_encoder = LabelEncoder()\n",
    "climate_df['Encoded Magnitude Scale'] = label_encoder.fit_transform(climate_df['Magnitude Scale'])\n",
    "climate_df['Encoded Disaster Type'] = label_encoder.fit_transform(climate_df['Disaster Type'])\n",
    "climate_df['Encoded Weather Type'] = label_encoder.fit_transform(climate_df['Weather Type'])\n",
    "\n",
    "training_mse = []\n",
    "training_cod = []\n",
    "mse_values = []\n",
    "cod_values = []\n",
    "# Prepare features and labels\n",
    "y = climate_df[\"Magnitude\"]\n",
    "X = climate_df.drop(columns=[\"Magnitude\", \"Magnitude Scale\",\"Disaster Type\", \"Weather Type\"], axis=1)\n",
    "\n",
    "N_RUNS = 50\n",
    "TRAIN_SIZE = 0.9\n",
    "random_state = 777\n",
    "\n",
    "X = np.ascontiguousarray(X, dtype=np.float64)\n",
    "y = np.ascontiguousarray(y, dtype=np.float64)\n",
    "\n",
    "N_RUNS = 50\n",
    "\n",
    "# Train with Random Forest Regression\n",
    "rf = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "# cross validation\n",
    "for i in range(N_RUNS):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=TRAIN_SIZE,\n",
    "                                                        random_state=random_state)\n",
    "    random_state += 777\n",
    "\n",
    "    # training\n",
    "    with config_context(assume_finite=True):\n",
    "        model = rf.fit(X_train, y_train)\n",
    "\n",
    "    # inference\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    training_mse.append(mean_squared_error(y_train, y_train_pred))\n",
    "    training_cod.append(r2_score(y_train, y_train_pred))    \n",
    "    \n",
    "    mse_values.append(mean_squared_error(y_test, y_pred))\n",
    "    cod_values.append(r2_score(y_test, y_pred))\n",
    "\n",
    "# Save the model to a file\n",
    "joblib.dump(model, 'model/waverX-Analysis.pkl')\n",
    "print( y_train_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226be709-59f9-4b8a-b822-1be621150b61",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "747fa311-2f01-4bee-a3d5-7c334976ff3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Distributing <class 'list'> object. This may take some time.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Training MSE</th>\n",
       "      <th>Test MSE</th>\n",
       "      <th>Training R2</th>\n",
       "      <th>Test R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>1.009342</td>\n",
       "      <td>3.968384</td>\n",
       "      <td>0.957698</td>\n",
       "      <td>-22.413748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Method Training MSE  Test MSE Training R2    Test R2\n",
       "0  Random Forest     1.009342  3.968384    0.957698 -22.413748"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rf_train_mse = sum(training_mse)/len(training_mse)\n",
    "rf_train_r2 = sum(training_cod)/len(training_cod)\n",
    "\n",
    "rf_test_mse = sum(mse_values)/len(mse_values)\n",
    "rf_test_r2 = sum(cod_values)/len(cod_values)\n",
    "\n",
    "rf_result = pd.DataFrame(['Random Forest', rf_train_mse, rf_test_mse, rf_train_r2, rf_test_r2]).transpose()\n",
    "rf_result.columns = [\"Method\", \"Training MSE\", \"Test MSE\", \"Training R2\", \"Test R2\"]\n",
    "rf_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764e0ca7-7be8-4050-ae6c-f5227e563f11",
   "metadata": {},
   "source": [
    "## Data Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2a9e1264-54ef-4eb9-8c73-91c0d14a563b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.cbook' has no attribute '_unpack_to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_176/4192514719.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create the scatter plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Scatter plot with blue circles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Customize the plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[1;32m   2815\u001b[0m \u001b[0;31m# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2816\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquiver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2817\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mquiver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2818\u001b[0m     __ret = gca().quiver(\n\u001b[1;32m   2819\u001b[0m         *args, **({\"data\": data} if data is not None else {}), **kw)\n",
      "\u001b[0;32m/opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mgca\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m   2270\u001b[0m     \u001b[0mNote\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mplotfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mintended\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mconvenience\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquickly\u001b[0m \u001b[0mplotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflat\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mintended\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0man\u001b[0m \u001b[0malternative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2272\u001b[0;31m     \u001b[0minterface\u001b[0m \u001b[0mto\u001b[0m \u001b[0mgeneral\u001b[0m \u001b[0mplotting\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2273\u001b[0m     \"\"\"\n\u001b[1;32m   2274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mgca\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gridspecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1567\u001b[0;31m         \u001b[0;31m# Create array to hold all axes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1568\u001b[0m         \u001b[0maxarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    770\u001b[0m                 \u001b[0;31m# gridspec only.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiglb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_suptitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layoutbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m                         layoutbox.vstack([self._suptitle._layoutbox,\n\u001b[1;32m    774\u001b[0m                                           child],\n",
      "\u001b[0;32m/opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages/matplotlib/axes/_subplots.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSubplotSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_subplotspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages/matplotlib/_api/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, rect, facecolor, frameon, sharex, sharey, label, xscale, yscale, box_aspect, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mprimarily\u001b[0m \u001b[0mused\u001b[0m \u001b[0mby\u001b[0m \u001b[0mrectilinear\u001b[0m \u001b[0mprojections\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m             \u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mmeant\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0moverridden\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m             \u001b[0mnew\u001b[0m \u001b[0mkinds\u001b[0m \u001b[0mof\u001b[0m \u001b[0mprojection\u001b[0m \u001b[0maxes\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mneed\u001b[0m \u001b[0mdifferent\u001b[0m \u001b[0mtransformations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             and limits. (See `~matplotlib.projections.polar.PolarAxes` for an\n",
      "\u001b[0;32m/opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mcla\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m             raise TypeError(\"Cannot supply both positional and keyword \"\n\u001b[0;32m-> 1210\u001b[0;31m                             \"arguments to this method.\")\n\u001b[0m\u001b[1;32m   1211\u001b[0m         \u001b[0;31m# Can't do `args == (None,)` as that crashes cycler.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mset_xlim\u001b[0;34m(self, left, right, emit, auto, xmin, xmax)\u001b[0m\n\u001b[1;32m   3694\u001b[0m         \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3695\u001b[0m         \u001b[0mBy\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMatplotlib\u001b[0m \u001b[0msupports\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mabove\u001b[0m \u001b[0mmentioned\u001b[0m \u001b[0mscales\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3696\u001b[0;31m         \u001b[0mAdditionally\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom\u001b[0m \u001b[0mscales\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mregistered\u001b[0m \u001b[0musing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3697\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_scale\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThese\u001b[0m \u001b[0mscales\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mthen\u001b[0m \u001b[0malso\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3698\u001b[0m         \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0mhere\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_process_unit_info\u001b[0;34m(self, datasets, kwargs, convert)\u001b[0m\n\u001b[1;32m   2524\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtitles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2525\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2526\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2527\u001b[0m             \u001b[0;31m# need to start again in case of window resizing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2528\u001b[0m             \u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mupdate_units\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1441\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0massumed\u001b[0m \u001b[0myou\u001b[0m \u001b[0mwant\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0mon\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mset\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m             \u001b[0mIf\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mno\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mtoggles\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1444\u001b[0m             \u001b[0mvisibility\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages/matplotlib/units.py\u001b[0m in \u001b[0;36mget_converter\u001b[0;34m(self, x)\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib.cbook' has no attribute '_unpack_to_numpy'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.scatter(y_train_pred, y_train, color='b', marker='o')  # Scatter plot with blue circles\n",
    "\n",
    "# Customize the plot\n",
    "plt.ylabel(\"Predicted Disasters Magnitude\")\n",
    "plt.xlabel(\"Past Disasters Magnitude\")\n",
    "plt.title(\"Scatter Plot of Predicted vs. Actual Disasters Magnitude\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9694ab6-9e1a-493d-be6b-57d71c3e7aee",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f34603d-35fc-4884-a1d4-b610db3b1d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pulled successfully\n",
      "waverX-Analysis model prediction for Flood\n",
      "Minimum predicted magnitude: 480 Km2\n",
      "Maximum predicted magnitude: 721 Km2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages/pandas/core/frame.py:5047: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "/opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages/ipykernel_launcher.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages/ipykernel_launcher.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages/ipykernel_launcher.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/app-root/miniconda3/envs/oneAPI-AIKit-MLPackage/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import requests\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "Run inference on model\n",
    "\"\"\"\n",
    "\n",
    "# Log server\n",
    "# Configure the logging settings\n",
    "logging.basicConfig(filename='waverx_analysis.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s')\n",
    "\n",
    "# Create a logger instance\n",
    "logger = logging.getLogger('waverx_nlp')\n",
    "\n",
    "\n",
    "\n",
    "# Load the model from file\n",
    "model = joblib.load('model/waverX-Analysis.pkl')\n",
    "\n",
    "# Set test Visual crossing weather API key\n",
    "os.environ[\"API_KEY\"]=\"R9M7HHCH4EDADEUBCU3ZENKXN\"\n",
    "\n",
    "def fetch_climate_data(location, start_date, end_date, disaster_type, api_key):\n",
    "    \"\"\"\n",
    "    Fetching climate data at specified time to run inference\n",
    "    \"\"\"\n",
    "    API_URL = 'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/'\n",
    "    \n",
    "    start_date = \"{}T00:00:00Z\".format(start_date)\n",
    "    end_date = \"{}T23:59:59Z\".format(end_date)\n",
    "    \n",
    "    if not api_key:\n",
    "        api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "    params = {\n",
    "        \"location\": location,\n",
    "        \"startTime\": start_date,  # start date of disaster\n",
    "        \"endTime\": end_date,  # end date of disaster\n",
    "        \"key\": api_key,\n",
    "        \"include\": \"days\",\n",
    "        \"contentType\": \"csv\",\n",
    "        \"elements\": \"tempmax,tempmin,dew,temp,windspeedmin,windspeedmax,windspeedmean,winddir,humidity,conditions,precip,cloudcover,pressure,precipcover\"\n",
    "    }\n",
    "    climate_data_req = requests.get(API_URL, params=params)\n",
    "    if climate_data_req.status_code != 200:\n",
    "        print(\"Fetching climate data failed\")\n",
    "        print(climate_data_req.text)\n",
    "        return climate_data_req.text\n",
    "    print(\"Data pulled successfully\")\n",
    "    climate_data_req = climate_data_req.text\n",
    "    climate_data_csv = StringIO(climate_data_req)\n",
    "    data_df = pd.read_csv(climate_data_csv)\n",
    "    # Filter specific columns\n",
    "    filtered_df = data_df[[\"tempmax\", \"tempmin\", \"dew\", \"temp\", \"windspeedmin\", \"windspeedmax\",\n",
    "                          \"windspeedmean\", \"winddir\", \"humidity\", \"conditions\", \"precip\", \"cloudcover\", \"sealevelpressure\", \"precipcover\"]]\n",
    "    filtered_df.insert(0, \"Disaster Type\", disaster_type)\n",
    "    # Calculate relative humidty min, max and mean\n",
    "    humidity_min = filtered_df[\"humidity\"] - 2\n",
    "    humidity_max = filtered_df[\"humidity\"] + 2\n",
    "    humidity_mean = (humidity_max + humidity_min) / 2\n",
    "    \n",
    "    # Set data columns to dataset columns\n",
    "    dataset_columns = {\"tempmin\": \"Minimum Temperature\", \"tempmax\": \"Maximum Temperature\", \"dew\": \"Dew Point\", \"temp\": \"Temperature\", \"windspeedmin\": \"Wind Speed Min\",\n",
    "               \"windspeedmax\": \"Wind Speed Max\", \"windspeedmean\": \"Wind Speed Mean\", \"winddir\": \"Wind Direction\", \"conditions\": \"Weather Type\",\n",
    "               \"precip\": \"Precipitation\", \"cloudcover\": \"Cloud Cover\", \"sealevelpressure\": \"Sea Level Pressure\", \"precipcover\": \"Precipitation Cover\"}\n",
    "    \n",
    "    filtered_df.rename(columns=dataset_columns, inplace=True)\n",
    "    filtered_df[\"Relative Humidity Min\"] = humidity_min\n",
    "    filtered_df[\"Relative Humidity Max\"] = humidity_max\n",
    "    filtered_df[\"Relative Humidity Mean\"] = humidity_mean\n",
    "    filtered_df.drop(columns=[\"humidity\"], inplace=True)\n",
    "    return filtered_df\n",
    "\n",
    "def predict(location, start_date, end_date, disaster_type, api_key):\n",
    "    climate_data_df = fetch_climate_data(location, start_date, end_date, disaster_type, api_key)\n",
    "    if type(climate_data_df) == str:\n",
    "        print(\"Prediction failed\")\n",
    "        return climate_data_df\n",
    "    \n",
    "    if disaster_type == \"Earthquake\":\n",
    "        climate_data_df[\"Magnitude Scale\"] = \"Richter\"\n",
    "    elif disaster_type == \"Storm\":\n",
    "        climate_data_df[\"Magnitude Scale\"] = \"Kph\"\n",
    "    else:\n",
    "        climate_data_df[\"Magnitude Scale\"] = \"Km2\"\n",
    "    # Encode  string values\n",
    "    label_encoder = LabelEncoder()\n",
    "    climate_data_df['Encoded Magnitude Scale'] = label_encoder.fit_transform(climate_data_df['Magnitude Scale'])\n",
    "    climate_data_df['Encoded Disaster Type'] = label_encoder.fit_transform(climate_data_df['Disaster Type'])\n",
    "    climate_data_df['Encoded Weather Type'] = label_encoder.fit_transform(climate_data_df['Weather Type'])\n",
    "    \n",
    "    # Drop NaN values\n",
    "    climate_data_df.dropna(inplace=True)\n",
    "    \n",
    "    X = climate_data_df.drop(columns=[\"Disaster Type\", \"Weather Type\",  \"Magnitude Scale\"], axis=1)\n",
    "\n",
    "    \n",
    "    predictions = model.predict(X)\n",
    "    converted_pred = []\n",
    "    if disaster_type == \"Flood\":\n",
    "        # Multiply by 1600000 and divide maximum sea level pressure to get km2 value\n",
    "        for mag in predictions:\n",
    "            mag = round((mag * 1600000) / max(climate_data_df['Sea Level Pressure']))\n",
    "            converted_pred.append(mag)\n",
    "    elif disaster_type == \"Storm\":\n",
    "        # Multiply by 30 to get Kph value\n",
    "        for mag in predictions:\n",
    "            mag = round(mag * 300)\n",
    "            converted_pred.append(mag)\n",
    "    elif disaster_type == \"Earthquake\":\n",
    "        # Multiply by 9 to get richter value\n",
    "        for mag in predictions:\n",
    "            mag = round(mag * 9)\n",
    "            converted_pred.append(mag)\n",
    "    else:\n",
    "        converted_pred = predictions\n",
    "    num_array = np.array(converted_pred)\n",
    "    min_pred = np.min(num_array)\n",
    "    max_pred = np.max(num_array)\n",
    "    print(f\"waverX-Analysis model prediction for {disaster_type}\")\n",
    "    print(f\"Minimum predicted magnitude: {min_pred} {climate_data_df['Magnitude Scale'][0]}\")\n",
    "    print(f\"Maximum predicted magnitude: {max_pred} {climate_data_df['Magnitude Scale'][0]}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Location in Latitude,Longitude format\n",
    "    # Dates in yyyy-mm-dd format\n",
    "    predict(\"80.41,85.9\", start_date=\"2023-01-16\", end_date=\"2023-01-17\", disaster_type=\"Flood\", api_key=os.getenv(\"API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b1831f-6143-4b68-b648-d374d772ee60",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3aa93c8b-018f-43e0-a384-6d54e6dbeb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "\"\"\" Flask Application \"\"\"\n",
    "\n",
    "from os import environ\n",
    "from flask import Flask, jsonify, request, make_response\n",
    "from flask_cors import CORS\n",
    "import  prediction\n",
    "import json\n",
    "\n",
    "# Configure the logging settings\n",
    "logging.basicConfig(filename='waverx_analysis.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s')\n",
    "# Create a logger instance\n",
    "logger = logging.getLogger('waverx_analysis')\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.config['JSONIFY_PRETTYPRINT_REGULAR'] = True\n",
    "cors = CORS(app, resources={r\"/api/v1/*\": {\"origins\": \"*\"}})\n",
    "\n",
    "@app.route(\"/api/v1/analysis/model/waverx\", methods=['POST'], strict_slashes=False)\n",
    "def model_inference():\n",
    "    location = request.form['location']\n",
    "    start_date = request.form['startDate']\n",
    "    end_date = request.form['endDate']\n",
    "    disaster_type = request.form['disasterType']\n",
    "    key = request.form[\"apiKey\"]\n",
    "    return jsonify(prediction.predict(location, start_date, end_date, disaster_type, key))\n",
    "\n",
    "@app.route(\"/api/v1/analysis/model/waverx/status\", strict_slashes=False)\n",
    "def model_status():\n",
    "    return jsonify({\"status\": \"OK\"})\n",
    "\n",
    "@app.errorhandler(404)\n",
    "def not_found(error):\n",
    "    \"\"\" 404 Error\n",
    "    ---\n",
    "    responses:\n",
    "      404:\n",
    "        description: a resource was not found\n",
    "    \"\"\"\n",
    "    return make_response(jsonify({'error': \"Not found\"}), 404)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Main Function \"\"\"\n",
    "    host = environ.get('MODEL_HOST')\n",
    "    port = environ.get('MODEL_PORT')\n",
    "    if not host:\n",
    "        host = '0.0.0.0'\n",
    "    if not port:\n",
    "        port = '5000'\n",
    "    app.run(host=host, port=port, threaded=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8a042483-f1b3-4389-b5c0-fe37d37250c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aioredis @ file:///tmp/build/80754af9/aioredis_1621433585469/work\n",
      "async-timeout @ file:///tmp/build/80754af9/async-timeout_1637851218186/work\n",
      "attrs @ file:///opt/conda/conda-bld/attrs_1642510447205/work\n",
      "backcall @ file:///home/conda/feedstock_root/build_artifacts/backcall_1592338393461/work\n",
      "backports.functools-lru-cache @ file:///home/conda/feedstock_root/build_artifacts/backports.functools_lru_cache_1618230623929/work\n",
      "bokeh @ file:///tmp/abs_34854e1f-d7d3-4f22-85d9-1075588e4ecdga64o0qg/croots/recipe/bokeh_1658136654619/work\n",
      "Bottleneck @ file:///tmp/build/80754af9/bottleneck_1648028896657/work\n",
      "certifi==2022.9.14\n",
      "charset-normalizer==3.3.2\n",
      "click==8.0.3\n",
      "cloudpickle @ file:///tmp/build/80754af9/cloudpickle_1598884132938/work\n",
      "colorama @ file:///tmp/build/80754af9/colorama_1607707115595/work\n",
      "coverage @ file:///tmp/build/80754af9/coverage_1614614864940/work\n",
      "cycler @ file:///home/conda/feedstock_root/build_artifacts/cycler_1635519461629/work\n",
      "Cython==0.29.25\n",
      "cytoolz==0.11.0\n",
      "dask==2021.10.0\n",
      "debugpy @ file:///tmp/build/80754af9/debugpy_1637091426235/work\n",
      "decorator @ file:///home/conda/feedstock_root/build_artifacts/decorator_1641555617451/work\n",
      "distributed @ file:///tmp/build/80754af9/distributed_1635968201545/work\n",
      "entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1643888246732/work\n",
      "filelock @ file:///opt/conda/conda-bld/filelock_1647002191454/work\n",
      "Flask==1.1.2\n",
      "Flask-Cors==4.0.0\n",
      "fonttools==4.25.0\n",
      "fsspec @ file:///opt/conda/conda-bld/fsspec_1647268051896/work\n",
      "grpcio @ file:///tmp/build/80754af9/grpcio_1614883945333/work\n",
      "gunicorn==19.9.0\n",
      "HeapDict @ file:///Users/ktietz/demo/mc3/conda-bld/heapdict_1630598515714/work\n",
      "hiredis @ file:///tmp/build/80754af9/hiredis_1621433282636/work\n",
      "idna==3.4\n",
      "importlib-metadata==6.7.0\n",
      "ipykernel @ file:///home/conda/feedstock_root/build_artifacts/ipykernel_1663104864651/work\n",
      "ipython @ file:///home/conda/feedstock_root/build_artifacts/ipython_1651240553635/work\n",
      "itsdangerous==2.1.2\n",
      "jedi @ file:///home/conda/feedstock_root/build_artifacts/jedi_1659959867326/work\n",
      "Jinja2 @ file:///tmp/build/80754af9/jinja2_1624781299557/work\n",
      "joblib==1.3.2\n",
      "jupyter-client @ file:///home/conda/feedstock_root/build_artifacts/jupyter_client_1633454794268/work\n",
      "jupyter_core @ file:///home/conda/feedstock_root/build_artifacts/jupyter_core_1658332345782/work\n",
      "kernda==0.3.0\n",
      "kiwisolver @ file:///opt/conda/conda-bld/kiwisolver_1653292039266/work\n",
      "locket @ file:///opt/conda/conda-bld/locket_1652903118915/work\n",
      "MarkupSafe==2.1.3\n",
      "matplotlib==3.5.3\n",
      "matplotlib-inline @ file:///home/conda/feedstock_root/build_artifacts/matplotlib-inline_1660814786464/work\n",
      "mkl-fft==1.3.1\n",
      "mkl-random==1.2.2\n",
      "mkl-service==2.4.0\n",
      "mkl-umath==0.1.1\n",
      "modin @ file:///home/hex/miniconda3/conda-bld/modin-packages_1644400528447/work\n",
      "msgpack @ file:///opt/conda/conda-bld/msgpack-python_1652362659880/work\n",
      "munkres==1.1.4\n",
      "nest-asyncio @ file:///home/conda/feedstock_root/build_artifacts/nest-asyncio_1648959695634/work\n",
      "numexpr==2.7.3\n",
      "numpy==1.21.6\n",
      "olefile @ file:///Users/ktietz/demo/mc3/conda-bld/olefile_1629805411829/work\n",
      "omniscidbe==0.1\n",
      "packaging @ file:///tmp/build/80754af9/packaging_1637314298585/work\n",
      "pandas==1.3.4\n",
      "parso @ file:///home/conda/feedstock_root/build_artifacts/parso_1638334955874/work\n",
      "partd @ file:///opt/conda/conda-bld/partd_1647245470509/work\n",
      "pexpect @ file:///home/conda/feedstock_root/build_artifacts/pexpect_1602535608087/work\n",
      "pickle5 @ file:///tmp/build/80754af9/pickle5_1621428040037/work\n",
      "pickleshare @ file:///home/conda/feedstock_root/build_artifacts/pickleshare_1602536217715/work\n",
      "Pillow @ file:///tmp/build/80754af9/pillow_1625670624344/work\n",
      "prompt-toolkit @ file:///home/conda/feedstock_root/build_artifacts/prompt-toolkit_1662384672173/work\n",
      "protobuf==3.17.2\n",
      "psutil @ file:///tmp/build/80754af9/psutil_1612298016854/work\n",
      "ptyprocess @ file:///home/conda/feedstock_root/build_artifacts/ptyprocess_1609419310487/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl\n",
      "pyarrow==3.0.0\n",
      "Pygments @ file:///home/conda/feedstock_root/build_artifacts/pygments_1660666458521/work\n",
      "pyparsing @ file:///tmp/build/80754af9/pyparsing_1635766073266/work\n",
      "python-dateutil==2.8.2\n",
      "pytz==2021.3\n",
      "PyYAML==6.0\n",
      "pyzmq==19.0.2\n",
      "ray==1.6.0\n",
      "redis @ file:///Users/ktietz/demo/mc3/conda-bld/redis-py_1629466587884/work\n",
      "requests==2.31.0\n",
      "scikit-ipp==1.2.0\n",
      "scikit-learn==1.0\n",
      "scipy==1.6.2\n",
      "setproctitle==1.1.10\n",
      "six @ file:///tmp/build/80754af9/six_1644875935023/work\n",
      "sortedcontainers @ file:///tmp/build/80754af9/sortedcontainers_1623949099177/work\n",
      "TBB==0.2\n",
      "tblib @ file:///Users/ktietz/demo/mc3/conda-bld/tblib_1629402031467/work\n",
      "threadpoolctl @ file:///Users/ktietz/demo/mc3/conda-bld/threadpoolctl_1629802263681/work\n",
      "toolz @ file:///tmp/build/80754af9/toolz_1636545406491/work\n",
      "tornado @ file:///tmp/build/80754af9/tornado_1606942283357/work\n",
      "traitlets @ file:///home/conda/feedstock_root/build_artifacts/traitlets_1663005918942/work\n",
      "typing_extensions @ file:///opt/conda/conda-bld/typing_extensions_1647553014482/work\n",
      "urllib3==2.0.7\n",
      "wcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1600965781394/work\n",
      "Werkzeug==2.2.3\n",
      "xgboost==1.4.2\n",
      "zict==2.1.0\n",
      "zipp==3.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "78148a4e-8d13-440a-b6ed-056f913cacda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: matplotlib 3.5.3\n",
      "Uninstalling matplotlib-3.5.3:\n",
      "  Successfully uninstalled matplotlib-3.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall matplotlib -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9b9da04b-fd4c-4801-ae0c-78e2ee3830e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_176/2821689564.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Y' is not defined"
     ]
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ce5ddbff-eb68-4f5c-a2bc-4df98e0e4467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping numpy as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall numpy -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bca066-347d-44ff-b868-78ff3cb97e45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Intel SKLearn, XGBoost, & Modin",
   "language": "python",
   "name": "oneapi-aikit-mlpackage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
